{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdMTmAw0_U6t"
   },
   "source": [
    "# Job Reccer Project - Job Search using Deep Learning Bi-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboardX in /home/jibc/.local/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from tensorboardX) (1.20.3)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/jibc/.local/lib/python3.9/site-packages (from tensorboardX) (3.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edited_roberta import *\n",
    "from run_classifier import evaluate, load_and_cache_examples, accuracy, set_seed\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup, AdamW,RobertaConfig,RobertaTokenizer)\n",
    "from utils import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our Bi-Encoder model\n",
    "\n",
    "In the following block, we'll create our neural network bi-encoder model. To do so, we'll define a new class `JobSearchBiencoderModel` that contains the two encoders, which are the encoder for query and the encoder for code. \n",
    "\n",
    "In pytorch, neural networks are defined by specifying their _parameters_ (the things that get updated during training) and a `forward` function that determines how to turn the inputs into outputs. For our bi-encoder, we'll need to fill these in as follows:\n",
    "\n",
    "* Specify the two encoders in the `__init__` function as fields of the class (e.g., `self.x = 1` makes `x` a field of the object), which will tell PyTorch that we'll be updating their parameters during training\n",
    "* Write the `forward` function so that we...\n",
    "  * encode the query as a vector\n",
    "  * encode the code-document as a vector\n",
    "  * compute the cosine similarity of the two vectors (where 1 is relevant, 0 is not-relevant)\n",
    "  \n",
    "We'll detail these next.\n",
    "\n",
    "## Creating the encoders\n",
    "\n",
    "How do we instantiate an encoder? There are two steps. First we need to figure out what is the _architecture_ of the model. This defines things like how many layers are in a neural network and how the layers are connected. In our case, _both_ of our encoders will use the RoBERTa architecture; as you might have guessed, RoBERTa is related to BERT and just has slightly different tweaks. There are [many BERT variants](https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23) and for the purposes of this excise, you can safely think of RoBERTa as the same as BERT.\n",
    "\n",
    "Second, once we have our model architecture, we need to specify which parameters we'll start with. You can think of the difference between the model architecture and parameters as if you were specifying a meal: The architecture is a bit like specifying the plates/bowls/container based on what kind of food you want and the parameters are like filling the container with a specific kind of food. There are many pre-trained sets of parameters for architectures, so a neural network starts with some existing knowledge of certain kinds of things (e.g., what human  language looks like,  what programming languages look like, or how to classify images). In neural network land, the [Huggingface Model Repository](https://huggingface.co/models) is a common place to look for parameters that people have shared with others.\n",
    "\n",
    "Returning to our IR problem, in our setting (conveniently), both of our encoders will use the same architecture: `RobertaModel`. If you _really_ want to know more, the code for this is provided in `edited_roberta.py`, but many end-users of these models (like us) will never need to look at this kind of code--and you certainly don't to complete this homework!\n",
    "\n",
    "## Writing the forward function \n",
    "\n",
    "The `forward` function defines how the neural network goes from inputs to outputs. In our case, we're going to feed the different inputs (query and document) to separate encoders and then compare the outputs. \n",
    "\n",
    "\n",
    "Your task here:\n",
    "1. Define the two encoders(query encoder and job_encoder) which are RobertaModel loaded from edit_roberta.py. You should two arguments when creating the instance of the Roberta Model, which are config and add_pooling_layer(equal to False).\n",
    "2. define and calculate the cosine similarity between two embeddings.\n",
    "3. define the loss function and loss.\n",
    "\n",
    "### TODOs in this block are worth 20 points total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobSearchBiencoderModel(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        ## TODO: \n",
    "        # Fill in the following parts where you specify each encoder's architecture. \n",
    "        # You'll need to pass in \"config\" as an argument to the architecture's constructor\n",
    "        # so it knows how to set things up.\n",
    "        #\n",
    "        # NOTE 1: Notice that we haven't specified the *parameters* here, just the architecture.\n",
    "        # We'll fill in the parameters later\n",
    "        #\n",
    "        # NOTE 2: If you were ever curious how to do other kinds of non-text IR (e.g., images),\n",
    "        # this part is where you'd specify a different kind of encoder architecture, such as\n",
    "        # ResNet50 for encoding images. The rest of the code for this class would be mostly the same!\n",
    "        # (The one caveat is that both models need to produce vector representations of the same size)\n",
    "        \n",
    "        self.query_encoder = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.job_encoder = RobertaModel(config, add_pooling_layer=False)\n",
    "\n",
    "        # This is our loss function that determines how \"good\" our model's output is.\n",
    "        # We'll use this in the forward() function to evaluate the model's outputs and\n",
    "        # then return the predictions and loss.        \n",
    "        self.loss_fn = BCEWithLogitsLoss()\n",
    "        \n",
    "        # This will initialize weights and apply final processing\n",
    "        self.post_init()\n",
    " \n",
    "    def forward(\n",
    "        self,\n",
    "        query_token_ids: Optional[torch.LongTensor] = None,\n",
    "        job_token_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "       \n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "        outputs = self.query_encoder(\n",
    "            query_token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            \n",
    "        )\n",
    "        query_emb = outputs[0][:, 0, :]\n",
    "        \n",
    "        outputs_code = self.job_encoder(\n",
    "            job_token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "           \n",
    "        )\n",
    "        code_emb = outputs_code[0][:, 0, :]\n",
    "\n",
    "        # TODO: using the cosine_similarity function (imported above),\n",
    "        # compute the similarity of the query and code embeddings.\n",
    "        cosine_sim = cosine_similarity(query_emb, code_emb)\n",
    "                       \n",
    "        # TODO: use the self.loss_fn (our loss function) to measure how good/bad\n",
    "        # the predictions are. This function will return a value you should \n",
    "        # call \"loss\". You can see how to call the function here\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "        # \n",
    "        # NOTE: The \"labels\" input to this function is the ground truth\n",
    "        # relevance scores (labels) for each input. You'll want to compare\n",
    "        # the cosine similarities with these labels when calling the function\n",
    "        #\n",
    "        # NOTE 2: There are many kinds of loss functions so understanding how\n",
    "        # to call them and which order the arguments go in is important\n",
    "        loss = self.loss_fn(cosine_sim, labels)\n",
    "\n",
    "        # Finally, let's return some output! Pytorch has provided\n",
    "        # some structure for us to say what-is-what in the output\n",
    "        # values. We'll return our loss (how \"bad\" the model's prediction was)\n",
    "        # and the logits, which is our predictions. \n",
    "        #\n",
    "        # You don't need to worry about the other two outputs.\n",
    "        #\n",
    "        # NOTE: Normally, we'd be passing the cosine similarity through\n",
    "        # some non-linear function (like a sigmoid!) to get \"logits\" as\n",
    "        # our output values. However, in a bi-encoder, often you just \n",
    "        # return the cosine similarity as the logits, so the name is wrong\n",
    "        # but the value is what's expected. Later on, when we access the\n",
    "        # \"logits\" part of the output, remember these are the query-doc\n",
    "        # cosine similarity scores\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cosine_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Models and Training Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training deep learning models often involves _lots_ of hyperparameter decisions--not to mention a bunch of seemingly-random bookkeeping options for where and when to save things. We have defined these all for you (yay) but it's worth at least looking through to see what kinds of decisions you'll need to make. Most importantly, we've specified the number of epochs and the batch size (more on that later) so that the model trains quickly.\n",
    "\n",
    "You will eventually need to edit the input file name to use the full dataset. Everything else can stay the same, though you're welcome to try changing some things and seeing what happens once you've completed the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.txt\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Where to save things\n",
    "        self.data_dir = './data'\n",
    "        self.model_type = 'roberta'\n",
    "        self.model_name_or_path = 'microsoft/codebert-base'\n",
    "        self.task_name = 'codesearch'\n",
    "        self.output_dir = './models'\n",
    "        self.output_mode = 'codesearch'\n",
    "\n",
    "        # These are going to be your most common hyperparameters to change.\n",
    "        # If you want to do deep learning stuff, it's worth learning a bit\n",
    "        # about what they are and what they do.\n",
    "        self.train_batch_size = 16\n",
    "        self.eval_batch_size = 16\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 1e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3 # NOTE: Change this to 1 if debugging so it runs faster\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.n_gpu = 1\n",
    "        self.no_cuda = False\n",
    "\n",
    "        # These are mostly configuration options for which pieces to run\n",
    "        self.config_name = \"\"\n",
    "        self.tokenizer_name = \"\"\n",
    "        self.cache_dir = \"\"\n",
    "        self.max_seq_length = 200\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.do_predict = False\n",
    "        self.evaluate_during_training = False\n",
    "        self.do_lower_case = False\n",
    "\n",
    "        # How often we save things\n",
    "        self.logging_steps = 1000 \n",
    "        self.save_steps = 1000\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Ignore all of these\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "        self.local_rank = -1\n",
    "        self.server_ip = \"\"\n",
    "        self.server_port = \"\"\n",
    "        \n",
    "        # Input and output files.\n",
    "        #\n",
    "        # TODO: Change the training file to train_300k.txt when ready\n",
    "        #\n",
    "        self.train_file = \"train_10.txt\"# CHANGE ME WHEN READY TO TRAIN!!!!!\n",
    "        self.dev_file = \"valid.txt\"\n",
    "        self.test_file = \"test_data.txt\"\n",
    "        self.pred_model_dir = './models/checkpoint-best'\n",
    "        self.test_result_dir = './results/'\n",
    "\n",
    "args = Args()\n",
    "print(args.test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following blocks, we are starting to train our model. Here we will firstly define the train function.\n",
    "\n",
    "In the train function, we will define the procedure of training the model, the main steps are:\n",
    "1. Define the dataloader (to do). You should use the function DataLoader(). Three arguments are required for you to input, which are dataset, batch_size, sampler.\n",
    "2. set the gradient to zero and train the model using back propagation.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training process\n",
    "\n",
    "Let's see how the training procedure works! The code block below specifies how we'll train our model. There's one part for you to fill in that loads the data using the `DataLoader` class. The rest is helpful to understand how \n",
    "\n",
    "### TODO in this block is worth 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, optimizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "\n",
    "    # The sampler specifies how we should access the training data, which\n",
    "    # in this case is in a random order\n",
    "    train_sampler = RandomSampler(train_dataset, replacement=False,num_samples=20)\n",
    "    \n",
    "    # TODO: Initailize the DataLoader (https://pytorch.org/docs/stable/data.html)\n",
    "    # so that it \n",
    "    # - loads from the provided train_dataset \n",
    "    # - samples using our sampler\n",
    "    # - uses the specified batch size\n",
    "    #\n",
    "    # NOTE: The batch size is pretty important! This says how many examples to train on \n",
    "    # at one time. If you recall, we talked about Stocastic Gradient Descent (SGD) that\n",
    "    # updates based on one instance at a time (e.g., changing the dog t-shirt size after seeing one dog)\n",
    "    # versus Gradient Descent (GD) that updates after all the data. SGD is much faster to\n",
    "    # converge to the \"right\" parameters but can make many missteps. The batch size \n",
    "    # says we can look at more than one instance at a time in determining how to update\n",
    "    # our parameters (e.g., look at a few dogs at a time to determine how to best update \n",
    "    # the t-shirt size, rather than just one dog or all the dogs)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=5, sampler=train_sampler,\n",
    "           batch_sampler=None)\n",
    "    \n",
    "    # How many total steps we'll take\n",
    "    t_total = len(train_dataloader) //  args.num_train_epochs\n",
    "\n",
    "    # The scheduler helps decide how quickly to update the weights based on how much\n",
    "    # training data we've seen. \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, args.warmup_steps, t_total)\n",
    "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
    "    if os.path.exists(scheduler_last):\n",
    "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
    "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = args.start_step\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    best_acc = 0.0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Note that this \"train_iterator\" is just tdqm wrapper that prints out which\n",
    "    # epoch we're currently in.     \n",
    "    train_iterator = trange(args.start_epoch, int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    set_seed(args) \n",
    "    \n",
    "    # This tells pytorch that we're going to be changing the parameters so it needs\n",
    "    # to start keeping track of stuff\n",
    "    model.train()\n",
    "    for idx, _ in enumerate(train_iterator):\n",
    "        \n",
    "        # Keep train of the training loss (how \"bad\" the performance is) for this epohch\n",
    "        tr_loss = 0.0\n",
    "        \n",
    "        # For one epoch, loop over all the data, one batch at a time\n",
    "        for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {'query_token_ids': batch[0],\n",
    "                      'job_token_ids': batch[1],\n",
    "                      'labels': batch[3]}\n",
    "            \n",
    "            ouputs = model(**inputs)\n",
    "            loss = ouputs[0]        \n",
    "            \n",
    "            # Do the back propagration to figure out which parameters to change.\n",
    "            # It's that easy!\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            # Update the parameters of our model based on the gradient and whatever\n",
    "            # else the optimizer is keeping track of\n",
    "            optimizer.step() \n",
    "            scheduler.step()  \n",
    "            \n",
    "            # This sets the gradient to zero before doing next update so we don't\n",
    "            # accidentally update the model based on the last batch's performance\n",
    "            model.zero_grad() \n",
    "            global_step += 1\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                break\n",
    "\n",
    "        # Once we finish an epoch, evaluate the model on the development data and see\n",
    "        # how well it does. We'll use this information to decide which version of\n",
    "        # the parameters to use.\n",
    "        results = evaluate(args, model, tokenizer, checkpoint=str(args.start_epoch + idx))\n",
    "\n",
    "        # \n",
    "        # Save the model and if we've already saved it, overwrite that saved model with \n",
    "        # the newly-trained parameters\n",
    "        #\n",
    "        last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "        if not os.path.exists(last_output_dir):\n",
    "            os.makedirs(last_output_dir)\n",
    "        model_to_save = model.module if hasattr(model,\n",
    "                                                'module') else model \n",
    "        model_to_save.save_pretrained(last_output_dir)\n",
    "        logger.info(\"Saving model checkpoint to %s\", last_output_dir)\n",
    "        idx_file = os.path.join(last_output_dir, 'idx_file.txt')\n",
    "        with open(idx_file, 'w', encoding='utf-8') as idxf:\n",
    "            idxf.write(str(args.start_epoch + idx) + '\\n')\n",
    "\n",
    "        torch.save(optimizer.state_dict(), os.path.join(last_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(last_output_dir, \"scheduler.pt\"))\n",
    "        logger.info(\"Saving optimizer and scheduler states to %s\", last_output_dir)\n",
    "\n",
    "        step_file = os.path.join(last_output_dir, 'step_file.txt')\n",
    "        with open(step_file, 'w', encoding='utf-8') as stepf:\n",
    "            stepf.write(str(global_step) + '\\n')\n",
    "\n",
    "        # Optional part 1 goes here\n",
    "\n",
    "        #\n",
    "        # If this model is better (on the training data) than the models from any of the \n",
    "        # past checkpoints, then keep a separate record of that too\n",
    "        #\n",
    "        if (results['acc'] > best_acc):\n",
    "            best_acc = results['acc']\n",
    "            output_dir = os.path.join(args.output_dir, 'checkpoint-best')\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            torch.save(args, os.path.join(output_dir, 'training_{}.bin'.format(idx)))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the training environment\n",
    "\n",
    "This will get a few things ready for the model to train. You don't need to really do much in this block but it's worth seeing how it works if you want to train models in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA so we can run on the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# This code will help us if we restart training and want to pick back up where we left off\n",
    "args.start_epoch = 0\n",
    "args.start_step = 0\n",
    "checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
    "    args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
    "    args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
    "    idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
    "    with open(idx_file, encoding='utf-8') as idxf:\n",
    "        args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
    "\n",
    "    step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
    "    if os.path.exists(step_file):\n",
    "        with open(step_file, encoding='utf-8') as stepf:\n",
    "            args.start_step = int(stepf.readlines()[0].strip())\n",
    "    logger.info(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Setting up the bi-encoder model and its parameters\n",
    "\n",
    "### the TODO in this block is worth 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/jibc/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We'll specify some general configurations that tell the models what kind\n",
    "# of parameters to use and how to turn incoming text/code data into identifiers for \n",
    "# processing with the neural network \n",
    "#\n",
    "# We set num_labels = 1 because this is a regression class\n",
    "# (compared to a classification task with many class labels)\n",
    "num_labels = 5\n",
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base',\n",
    "                                      num_labels=num_labels, finetuning_task=args.task_name)\n",
    "\n",
    "# We'll treat relevance as a regression problem\n",
    "config.problem_type = 'regression'\n",
    "\n",
    "# If you remember from our neural language model part of the lecture, we talked\n",
    "# about one language model that gets fed a series of words to predict the next\n",
    "# and each word is mapped to an embedding. The \"tokenizer\" specifies how to \n",
    "# break up words into tokens but it frequently doesn't use just spaces!\n",
    "# In fact, most tokenizers break words into *pieces* to reduce the size of the\n",
    "# vocabularly (fewer embeddings!) so we need to specify which tokenizer to use.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "\n",
    "# Now comes the time to define our model. Let's specify the model class (which we'll need later)\n",
    "model_class = JobSearchBiencoderModel\n",
    "# And we'll instantiate the model itself.\n",
    "model = JobSearchBiencoderModel(config)\n",
    "\n",
    "# Now comes the magic where we specify the two encoders. Conveniently for us,\n",
    "# there's actually a very recent langauge model that knows *both* code and human language!!\n",
    "# We'll use this set of parameters to initialize *each* of our encoders. Over time,\n",
    "# each encoder's parameters will start to become different since one side is going\n",
    "# to learn how to encode queries better and the other will learn how to encode \n",
    "# code documents.\n",
    "#\n",
    "# NOTE: There's nothing stopping us from trying other parameters for the\n",
    "# encoders too. If you're feeling curious you could swap in any RoBERTa model\n",
    "# for the query encoder and it will just work.\n",
    "#\n",
    "# TODO: Initialize each of the coders using the \"from_pretrained\" method and\n",
    "# specifying the pretrained model you want. Here, we'll use the CodeBERT model, \n",
    "# which is hosted on Huggingface https://huggingface.co/microsoft/codebert-base\n",
    "# You should pass in the full name of the pretrained model (which includes the \"/\").\n",
    "# Note that this code is going to look the same for both encoders and may \n",
    "# seem kind of easy to do but we want you to see how to do it yourself. :) \n",
    "model.query_encoder = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.job_encoder =  RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# This will move the model's parameters onto the GPU so it runs fast\n",
    "model.to(args.device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "# Remember how we talked about stochastic gradient descent (SGD)? Well, it's not\n",
    "# the only way to update parameters. There are many (many) ways to do this\n",
    "# and the usual standard is actually AdamW which uses a bit of bookkeeping to figure\n",
    "# out how to update the weights more efficiently so the model learns faster.\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "# If we're restarting, load the optimizer's state at the last time step\n",
    "optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
    "if os.path.exists(optimizer_last):\n",
    "    optimizer.load_state_dict(torch.load(optimizer_last))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training!\n",
    "\n",
    "Finally!! Let's train that model and save it to a file so we can evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 01:57:11 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x14e8c0ffb610>\n",
      "12/14/2022 01:57:11 - INFO - run_classifier -   Loading features from cached file ./data/cached_train_train_10_codebert-base_200_codesearch\n",
      "12/14/2022 01:57:11 - INFO - __main__ -   ***** Running training *****\n",
      "12/14/2022 01:57:11 - INFO - __main__ -     Num examples = 5\n",
      "12/14/2022 01:57:11 - INFO - __main__ -     Num Epochs = 3\n",
      "12/14/2022 01:57:11 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "12/14/2022 01:57:11 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/14/2022 01:57:11 - INFO - __main__ -     Total optimization steps = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached_features_file ./data/cached_train_train_10_codebert-base_200_codesearch True\n",
      "<utils.InputFeatures object at 0x14e8054c7460>\n",
      "The trainint dataset looks like this:  <torch.utils.data.dataset.TensorDataset object at 0x14e8052a7c70>\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.64it/s]\u001b[A\n",
      "2it [00:00,  3.04it/s]\u001b[A\n",
      "3it [00:00,  4.25it/s]\u001b[A\n",
      "4it [00:00,  4.03it/s]\u001b[A\n",
      "12/14/2022 01:57:12 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_codebert-base_200_codesearch\n",
      "12/14/2022 01:57:12 - INFO - run_classifier -   Creating features from dataset file at ./data\n",
      "12/14/2022 01:57:12 - INFO - utils -   LOOKING AT ./data/valid.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached_features_file ./data/cached_dev_valid_codebert-base_200_codesearch False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 01:57:13 - INFO - utils -   Writing example 0 of 46213\n",
      "12/14/2022 01:57:13 - INFO - utils -   *** Example ***\n",
      "12/14/2022 01:57:13 - INFO - utils -   guid: dev-0\n",
      "12/14/2022 01:57:13 - INFO - utils -   query_token_ids: 0 6407 13851 37357 5 2324 828 2622 30 5222 41 3169 19470 463 36 192 20387 39891 463 37380 1215 347 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   code_token_ids: 0 9232 386 1215 9981 10845 36 1403 2156 2345 2156 1100 2156 425 5457 9291 2156 414 5457 9291 2156 17017 5457 9291 2156 923 5457 321 2156 1123 5457 883 612 4839 4832 18088 1403 479 18134 642 4345 1215 9981 10845 16 9291 2156 22 44307 554 48726 113 1403 479 18134 642 4345 1215 9981 10845 5457 221 4345 48132 36 2345 2156 1100 2156 425 2156 414 2156 17017 2156 923 2156 1123 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   label: 0 (id = 0)\n",
      "12/14/2022 01:57:13 - INFO - utils -   *** Example ***\n",
      "12/14/2022 01:57:13 - INFO - utils -   guid: dev-1\n",
      "12/14/2022 01:57:13 - INFO - utils -   query_token_ids: 0 47167 70 5 6773 11 10 576 31826 18099 129 6773 19 5 576 5064 114 17966 479 20 576 449 605 48204 32 1595 149 7 5 2340 47073 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   code_token_ids: 0 9232 3116 1215 12376 36 7425 1215 22609 2156 414 2156 1157 1215 48696 5457 7447 4839 4832 40462 5457 11988 479 2718 479 16 21710 36 7425 1215 22609 4839 16 8458 5457 16 48768 36 414 2156 889 4839 114 40462 8 45 1157 1215 48696 4832 1693 47617 36 128 19186 4345 34 57 6242 108 128 8 2870 7606 29 8785 108 7606 7425 1215 22609 4839 114 45 36 16 8458 50 16 48768 36 414 2156 1586 15453 4839 4839 4832 1693 47617 36 128 10836 129 3116 1586 15453 8720 50 36451 108 128 8204 7 7425 2870 955 4839 21634 5457 414 114 16 8458 1493 414 479 21634 114 45 70 36 16 48768 36 1615 2156 28700 4839 13 1615 11 21634 4839 4832 1693 47617 36 128 36583 4785 531 28 45073 5119 955 4839 7425 1215 2\n",
      "12/14/2022 01:57:13 - INFO - utils -   label: 0 (id = 0)\n",
      "12/14/2022 01:57:13 - INFO - utils -   *** Example ***\n",
      "12/14/2022 01:57:13 - INFO - utils -   guid: dev-2\n",
      "12/14/2022 01:57:13 - INFO - utils -   query_token_ids: 0 22376 7022 819 9 9188 46249 47184 36 27290 4839 634 5 39825 740 47555 20686 8 2740 30 22367 86 192 141 7 422 15 5936 111 516 1065 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   code_token_ids: 0 9232 4392 14386 36 326 119 21527 2156 326 119 45137 2156 295 33177 29 4839 4832 849 1045 27290 4327 7 2450 326 119 5457 326 119 21527 36 346 10643 18551 29 5457 326 119 45137 4839 849 5368 8135 414 414 5457 295 35187 479 9624 479 20979 2544 36 321 2156 132 2156 646 326 119 45137 2156 295 33177 29 27779 4839 479 12976 37356 36 128 46349 2881 108 4839 13 939 11 3023 9435 36 295 33177 29 4839 4832 849 92 414 358 86 6 42 16 5 2373 403 819 849 588 819 74 28 357 6 25 5 8135 414 74 45 28 2198 9624 385 5457 414 646 4832 2156 939 27779 849 5 3031 5043 7 4392 328 326 119 479 37357 36 385 2156 7447 4839 2 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   label: 1 (id = 1)\n",
      "12/14/2022 01:57:13 - INFO - utils -   *** Example ***\n",
      "12/14/2022 01:57:13 - INFO - utils -   guid: dev-3\n",
      "12/14/2022 01:57:13 - INFO - utils -   query_token_ids: 0 15953 368 2630 7 769 111 860 21013 1519 71 1996 5 3018 13 24790 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   code_token_ids: 0 9232 18134 26628 44298 36 48930 4839 4832 787 1531 3894 22890 479 18166 36 48930 4839 3816 8144 36 1009 49503 2156 1009 1009 449 605 48204 4839 4832 849 48639 6 2156 36106 14263 1434 6 3425 31 1423 16037 479 21748 6595 48639 849 114 1423 16037 16 145 422 786 8007 21574 6 172 52 393 5494 1506 6 53 52 849 109 486 48639 4 11515 2072 44518 6 98 14 10 27754 33000 64 28 7899 35 10813 5457 720 17075 479 120 36 128 8007 12228 108 4839 860 4832 671 48930 36 1009 49503 2156 1009 1009 449 605 48204 4839 4682 5034 479 18286 479 44454 30192 25 364 4832 114 364 479 1263 479 2194 1215 20414 45994 5034 479 14284 479 18893 4832 849 642 4360 2544 35 33022 5214 2362 12 8648 37764 479 47423 36 2\n",
      "12/14/2022 01:57:13 - INFO - utils -   label: 1 (id = 1)\n",
      "12/14/2022 01:57:13 - INFO - utils -   *** Example ***\n",
      "12/14/2022 01:57:13 - INFO - utils -   guid: dev-4\n",
      "12/14/2022 01:57:13 - INFO - utils -   query_token_ids: 0 20763 19099 36 321 479 321 111 112 479 321 4839 3543 227 32833 2351 106 114 2139 479 20 2408 3372 5 3585 9 5 2748 36 45 5 701 4839 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   code_token_ids: 0 9232 3438 1215 46840 36 1403 2156 13561 4839 4832 114 1403 479 34 1215 5282 36 13561 4839 4832 295 5457 1403 646 13561 27779 1403 479 32833 479 3438 36 295 4839 2424 1403 646 13561 27779 849 27336 70 15716 3329 13561 8 70 5678 7 24 4 13 364 11 889 36 1403 479 15716 4839 4832 114 295 11 36 364 479 37908 134 2156 364 479 37908 176 4839 4832 114 295 11 364 479 37908 134 479 5678 4832 364 479 37908 134 479 5678 479 3438 36 295 4839 114 295 11 364 479 37908 176 479 5678 4832 364 479 37908 176 479 5678 479 3438 36 295 4839 1403 479 15716 479 3438 36 364 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 01:57:13 - INFO - utils -   label: 0 (id = 0)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "12/14/2022 01:57:20 - INFO - utils -   Writing example 10000 of 46213\n",
      "12/14/2022 01:57:27 - INFO - utils -   Writing example 20000 of 46213\n",
      "12/14/2022 01:57:33 - INFO - utils -   Writing example 30000 of 46213\n",
      "12/14/2022 01:57:40 - INFO - utils -   Writing example 40000 of 46213\n",
      "12/14/2022 01:57:45 - INFO - run_classifier -   Saving features into cached file ./data/cached_dev_valid_codebert-base_200_codesearch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<utils.InputFeatures object at 0x14e7edfdbd30>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 01:57:49 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/14/2022 01:57:49 - INFO - run_classifier -     Num examples = 46213\n",
      "12/14/2022 01:57:49 - INFO - run_classifier -     Batch size = 16\n",
      "\n",
      "Evaluating:   0%|          | 0/2889 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/3 [00:37<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'code_token_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_561293/4136800268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Call the training function that we defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_561293/2888549246.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer, optimizer)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# how well it does. We'll use this information to decide which version of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# the parameters to use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/JobReccer/run_classifier.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, tokenizer, checkpoint, prefix, mode)\u001b[0m\n\u001b[1;32m    233\u001b[0m                           'labels': batch[3]}\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0mtmp_eval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'code_token_ids'"
     ]
    }
   ],
   "source": [
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Load in the training dataset. Here, we've handled most of the data preprocessing for you\n",
    "# train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, ttype='train')\n",
    "train_dataset = \n",
    "print(f'The trainint dataset looks like this: ', train_dataset)\n",
    "# train_dataset = \n",
    "print(len(train_dataset))\n",
    "# Call the training function that we defined above\n",
    "global_step, tr_loss = train(args, train_dataset, model, tokenizer, optimizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "# Save the trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "model_to_save.save_pretrained(args.output_dir)\n",
    "tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = AutoModel.from_pretrained(args.output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the best model on the test data\n",
    "\n",
    "Our code in training keeps track of how the model is doing and currently keeps around the files for the \"best performing\" model on the training data. How well does this model do on the test data? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "checkpoint = args.output_dir\n",
    "\n",
    "logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
    "\n",
    "print(checkpoint)\n",
    "global_step = \"\"\n",
    "model = model_class.from_pretrained(checkpoint)\n",
    "model.to(args.device)\n",
    "result = evaluate(args, model, tokenizer, checkpoint=checkpoint, prefix=global_step)\n",
    "result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "print(result)\n",
    "\n",
    "    \n",
    "# Optional part 3 goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qixrKejmJ-N5"
   },
   "source": [
    "# Doing Inference on the Test Dataset\n",
    "\n",
    "Finally, let's estimate the relevance scores for the query-document pairs in our test dataset. The test dataset **test_data.csv** contains each pair of 99 queries and 958 documents, which in total adds up to 94,842 query-document annotations (compare that with the project update number!). \n",
    "\n",
    "For ease of this exercise, we've already processed the data into a ready-to-go format in **test_data.txt** which is required by the model. In the following block, for each query-document pair, we generate a prediction score that measure the relevance of that pair by feeding the pair as inputs to the model's `forward` function (note that in pytorch if you have some model, doing `model(inputs)` and `model.forward(inputs)` is the same--it's trying to emphasize thinking of these as functions!). \n",
    "\n",
    "Once we have the model predictions, we'll create a new dataframe that contains (1) the query id, (2) the document id, and (3) the relevance score for that pair. We'll hand this dataframe off to Part 2 so you can finish up your GPU work.\n",
    "\n",
    "You should adjust the directory path accordingly in order to successfully do the inference. Check what you got in the result.txt file and write this back into the **test_data.csv** that adds an additional column \"sim\". Later In Part 2, you will incorporate the prediction score into the learning to rank model to see if it can improve the performance of ranking,\n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "This implementation works because we've aligned the `test_data.txt` and `test_data.csv` files so they're in the same order. That means you can write a long list of similarities and then add it back to the test data's DataFrame and it will Just Work™. However, in production settings, it's often useful to keep identifiers with the data as much as possible so that you don't just have a file of predictions and instead can write the predictions with the query/document identifiers (or whatever data you're working with).\n",
    "\n",
    "In this homework's setup, we're precomputed the relevance scores for later integration with some overall ranking function (done in Part 2). To get these, we re-encode everything for each step. In commercial systems, what is typically done is the documents are encoded once and then cached, much like how our inverted index caches the terms in each document. Then when a new queries arrives, we only have to encode it and compare it with the cached document emeddings. This saves a lot of time! Thankfully our dataset is quite small here so we don't need to do that, but the ability to precompute and cache embeddings is worth remembering why bi-encoders are helpful and efficient for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This tells the model that we're switching to evaluation mode (rather than training)\n",
    "# so it should turn off any training-specific functionality that could make it slower\n",
    "# or interfere with our results.\n",
    "model.eval()\n",
    "    \n",
    "# Note here: we're loading the test set ***in sequential order***. This is critical\n",
    "# for the next step because we need to map these predictions to query-document pairs.\n",
    "# In training, we want to see a random order, but typically not during test.\n",
    "eval_dataset, instances = load_and_cache_examples(args, \"jobsearch\", tokenizer, ttype='test')\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "# This data structures will have our predictions and we'll fill them as we process each batch\n",
    "relevance_predictions = np.array([])\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "    # Get the model's cosine similarity for the query-document pairs\n",
    "    # we pass as input. This no_grad() call also tells pytorch that\n",
    "    # we're doing evaluation so pytorch doesn't have to keep track\n",
    "    # of any gradients for updating the model (e.g., remember how\n",
    "    # in the dog t-shirt fitting, we had to keep around how much to change\n",
    "    # the t-shirt sizes).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Prepare the inputs\n",
    "        inputs = {'query_token_ids': batch[0],\n",
    "                  'job_token_ids': batch[1],\n",
    "                  'labels': batch[3]}\n",
    "       \n",
    "        # Note that this is a list of outputs, which includes the cosine\n",
    "        # similarity, among other stuff\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Let's pull out just the cosine similarity\n",
    "    _, cosine_sim = outputs[:2] \n",
    "    \n",
    "    # Pytorch works with \"tensors\" which are just like fancy numpy arrays.\n",
    "    # One main difference is that the tensor might \"live\" on a GPU, which \n",
    "    # means we need to copy it into regular computer memory to use it.\n",
    "    # Here, we'll call .cpu() to get the value back off the GPU and then\n",
    "    # convert the similarities to numpy. Remember, we're getting a list\n",
    "    # of similarities back out!\n",
    "    cosine_sims = cosine_sim.cpu().numpy()\n",
    "    \n",
    "    # Add these similarities to our current similarities\n",
    "    relevance_predictions = np.append(relevance_predictions, cosine_sims, axis=0)\n",
    "\n",
    "if not os.path.exists(args.test_result_dir):\n",
    "    os.makedirs(args.test_result_dir)\n",
    "\n",
    "output_test_file = os.path.join(args.test_result_dir, 'relevance-scores.csv')\n",
    "\n",
    "with open(output_test_file, \"w\") as outf:\n",
    "    logger.info(\"***** Writing relevance predictions *****\")\n",
    "    all_logits = relevance_predictions.tolist()\n",
    "    \n",
    "    # Note that we write these all as one big list. In the next step,\n",
    "    # we'll merge these with the data frame\n",
    "    outf.write(\",\".join([str(item) for item in all_logits]))\n",
    "    \n",
    "# Optional part 3 goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Merge the predictions with the query/doc pairs (10 points)\n",
    "\n",
    "The numpy array `relevance_predictions` now contains a list of all the similarities, which we'll need to merge with the test data. Conveniently, these predictions appear in the exact same order as the query-document pairs in the `data/test_data.csv` file.\n",
    "\n",
    "Your task is to read in `test_data.csv` as a dataframe and merge these relevance predictions as a new column called \"sim\". We'll export this dataframe to a separate file with just a few columns for better efficiency. Write a _new_ file with a subset of this dataframe containing only the columns\n",
    "* \"sim\"\n",
    "* \"qid\" (the query id)\n",
    "* \"docno\" (the document id)\n",
    "These last two columns match the pyterrier naming conventions, which you'll need for Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Optional TODO_: Evaluating the different models (20 points total; this is part 1)\n",
    "\n",
    " In the code above, we save the model's parameter for the most recent epoch and an extra directory for saving the model with the highest accuracy so far on the validation data. How much training does the model actually need to recognize relevance? Would one epoch be enough? What if we did 10? or 100? (100 might be too many for Great Lakes limits...). In this **optional part**, we'll describe a series of steps you can take to explore this part!\n",
    " \n",
    "Most of this optional part consists of changing or extending the code above using regular python/pandas things (no deep learning) so this is accessible to anyone. It will require you to figure out how some of the code does work though, so it's useful in general. \n",
    " \n",
    "Here's what you need to do:\n",
    "* Right now the model trains for 3 epochs total. Increase that number to 5 or more. There's a 3-hour limit per session for GPUs in Great Lakes so if you've completed all of part 1, it's worth getting a fresh session to get all 3 hours again. You can increase the number of epochs if you want too.\n",
    "* When training, we save the last checkpoint (overwriting the previous result) and also see if this is the \"best\" model and save that too. You will need to add more code here to save the model after every epoch. The code to do the saving is already shown in that block, so you'll need to figure out which parts to re-use _and_ be sure to change the directory. Look for \"Optional part 1 goes here\" on where to start\n",
    "* After training completes, let's see how well each of the models does on the test set. We've already provided some code that does the evaluation on the best performing model. Add more code so that it evaluates the models you just saved for each epoch (look for \"Optional part 2 goes here\") and make a plot of the performance on the test set\n",
    "* To measure the impact on NDCG, we'll need to calculate the different bi-encoders' relevance estimates to different files to use in Part 2. You'll need to add more code that loads in each of these models from the checkpoint directories and runs the inference\n",
    "* In the Part 4, just write the different models' predictions to separate files. You might combine this with the part 3 code if it's easier to do there. We haven't marked a spot for it explicitly, but you'll use these files in part 2.\n",
    "\n",
    "The output of Part 1 should be a plot showing the F1 performance per epoch and a list of files for each epoch's trained model's relevance predictions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3HDF3vRE017G93FTr+2MX",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb8a6753386f2b868fcf459bc0e992b5757180142da959a1670c98811a9fe0f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
