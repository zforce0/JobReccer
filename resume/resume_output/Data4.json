{
      "name": "SUMMARY \u2022",
      "email": "Kabilan.Kempraj@mechlintech.com",
      "mobile_number": "442-278-5514",
      "skills": [
            "Pivot tables",
            "Scheduling",
            "Communication",
            "Health",
            "Design",
            "Reporting",
            "Warehouse",
            "Scrum",
            "Xml",
            "Troubleshooting",
            "Analyze",
            "Hadoop",
            "Scripting",
            "Docker",
            "Api",
            "Business intelligence",
            "Python",
            "Analysis",
            "Pyspark",
            "Hive",
            "Rest",
            "Shell",
            "Github",
            "Testing",
            "Excel",
            "Process",
            "Database",
            "Visual",
            "Mysql",
            "Pivot",
            "Scala",
            "Networking",
            "Logistics",
            "Migration",
            "Etl",
            "Finance",
            "Sql server",
            "Computer science",
            "Oracle",
            "Analytics",
            "Hbase",
            "Architecture",
            "Automation",
            "Engineering",
            "R",
            "Programming",
            "Transactions",
            "Access",
            "Spark",
            "Presentation",
            "Json",
            "Sql",
            "Administration",
            "Tableau",
            "Datasets"
      ],
      "college_name": null,
      "degree": null,
      "designation": null,
      "experience": [
            "\u2022  Automated all the deployments using Terraform and DevOps tools like Git, Jenkins, JFrog, SonarQube and",
            "Azure Data Factory.",
            "Credit Derivative:",
            "\u2022  Data transition involving import and exports from multiple RDBMSs like Oracle and Teradata to Big Data",
            "technologies like Hive, HBase using Sqoop automation using Shell scripting.",
            "\u2022  Data cleaning using Spark Scala and Spark SQL with RDDs and Data frames to analyze datasets for potential",
            "data issues.",
            "Loaded data into Hive as a presentation layer on top of HBase for Analysts/Analytics.",
            "\u2022",
            "\u2022  Developed shell scripting for scheduling daily Sqoop jobs for data transitions.",
            "\u2022  Automated direct hot fixes on Production using CI/CD pipeline (Git, Jenkins, JFrog and Fortify).",
            "\u2022  Developed MapReduce jobs to parse raw data into tabular format and loaded it into HBase tables.",
            "\u2022  Query  optimization  in  Hive  by  modifying  legacy  table  structure  using  Partitioning,  Bucketing  and",
            "Compression techniques.",
            "\u2022  Redesigning legacy SQL functions into SparkSQL and HiveQL for fast and effective processing.",
            "\u2022  Created Hive external tables to store JSON and XML files using Serializers/Deserializers (JsonSerDe, XPath)",
            "\u2022  Active participation in daily scrum calls, business meetings, requirement gathering, test case development",
            "and job monitoring.",
            "All Management Corporation, Los Angeles-CA",
            "Role: Sr. Application Data Engineer",
            "Aug 2013\u2013 Feb 2015",
            "\u2022  Requirement gathering, business discussions and retrospectives meetings to provide quality output.",
            "\u2022  Code optimization using Spark to reduce runtime and enhance performance.",
            "\u2022",
            "Job log transition through Kafka data pipelines; Analyzed the logs using Spark Sink connectors to Kafka and",
            "Visualized using Grafana to identify potential failures.",
            "\u2022  Migrated  legacy  R  programs  to  Spark  framework  using  Scala  for  Data  cleaning  and  Identifying  fraud",
            "transactions.",
            "\u2022  Scheduled and monitored daily jobs in Tivoli workspace for automating daily execution.",
            "\u2022  Mentored and trained users to speed up and bring them aligned with modernization efforts.",
            "\u2022  Tableau report generation for business review and validation.",
            "\u2022  Documented and developed reusable Spark code templates and made them available in GitHub to initiate",
            "and the migration efforts across the organization.",
            "Implemented DevOps (CI/CD) model to automate traditional deployment process.",
            "\u2022",
            "EDUCATIONAL QUALIFICATIONS:",
            "\u2022  Master of Science in Computer Science and Computer Engineering, University of Arkansas, Fayetteville."
      ],
      "company_names": null,
      "no_of_pages": 3,
      "total_experience": 1.5
}